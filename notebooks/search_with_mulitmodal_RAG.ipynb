{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Retrieval Augmented Generation with Content Understanding\n",
    "\n",
    "# Overview\n",
    "\n",
    "Azure AI Content Understanding provides a powerful solution for extracting data from diverse content types, while preserving semantic integrity and contextual relationships ensuring optimal performance in Retrieval Augmented Generation (RAG) applications.\n",
    "\n",
    "This sample demonstrates how to leverage Azure AI's Content Understanding capabilities to extract:\n",
    "\n",
    "- OCR and layout information from documents\n",
    "- Audio transcription with speaker diarization from audio files\n",
    "- Shot detection, keyframe extraction, and audio transcription from videos\n",
    "\n",
    "This notebook illustrates how to extract content from unstructured multimodal data and apply it to Retrieval Augmented Generation (RAG). The resulting markdown output can be used with LangChain's markdown header splitter for semantic chunking. These chunks are then indexed in Azure AI Search. When a user submits a query, Azure AI Search retrieves relevant chunks to generate a context-aware response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario\n",
    "\n",
    "SecureHome Insurance, a leading property insurance company, faces a significant challenge following a recent natural disaster that has led to an influx of insurance claims. The data analyst at SecureHome Insurance is tasked with accurately validating ingested data from claims and invoices being processed through the system. These claims include various multimodal content types, such as policy plans (text documents), photos of property damage (images), footage of the disaster impact (videos), and recorded statements from insurance adjusters (audio files). The goal is to streamline the process and ensure analysts have all necessary information at their fingertips to maintain accuracy and compliance.\n",
    "\n",
    "To address this challenge, SecureHome Insurance uses Azure AI Content Understanding to create a unified system that extracts and analyzes data from multimodal sources. The system processes text documents to extract key information like policy details and invoice documents, analyzes images to assess the extent of property damage, processes videos to understand the impact of the disaster, and transcribes and analyzes audio files to capture adjuster reports and statements. By preserving semantic integrity and contextual relationships, the system ensures that all relevant information is accurately mapped to defined schemas such as policy plans, invoices, and insurance adjuster reports.\n",
    "\n",
    "In practice, when a data analyst receives a batch of insurance claims, they use the integrated platform to search for relevant information. The system employs a Retrieval-Augmented Generation (RAG) approach, where it first retrieves relevant data from text documents, images, videos, and audio files.  This retrieved data is then used to generate comprehensive and contextually accurate responses to the analyst's queries. \n",
    "\n",
    "By leveraging the RAG approach, the system ensures that the analyst has access to the most relevant and up-to-date information, enabling them to accurately validate and process the claims efficiently. This integration of Azure AI Content Understanding with RAG significantly enhances the claims processing system, leading to improved accuracy and efficiency in handling insurance claims.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample\n",
    "2. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt\n",
    "! pip install python-dotenv langchain langchain-community langchain-openai langchainhub openai tiktoken azure-identity azure-search-documents==11.6.0b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\") or \"2024-12-01-preview\"\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\") or \"2024-11-30\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\") or \"2024-08-01-preview\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\") or \"2023-05-15\"\n",
    "\n",
    "# Load and validate Azure Search Services configs\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-doc-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create analyzer with pre-defined schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "#CREATE ANALYZERS\n",
    "analyzer_configs = [\n",
    "    {\n",
    "        \"id\": \"doc-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/content_document.json\",\n",
    "        \"location\": Path(\"../data/sample_layout.pdf\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"image-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/content_image.json\",\n",
    "        \"location\": Path(\"../data/sample_image.png\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"audio-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/content_audio.json\",\n",
    "        \"location\": Path(\"../data/audio.wav\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"video-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/content_video.json\",\n",
    "        \"location\": Path(\"../data/FlightSimulator.mp4\"),\n",
    "    },\n",
    "]\n",
    "# Create Content Understanding client\n",
    "content_understanding_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/content_extraction\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Iterate through each analyzer and create it using the content understanding client\n",
    "for analyzer in analyzer_configs:\n",
    "    analyzer_id = analyzer[\"id\"]\n",
    "    template_path = analyzer[\"template_path\"]\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create the analyzer using the content understanding client\n",
    "        response = content_understanding_client.begin_create_analyzer(\n",
    "            analyzer_id=analyzer_id,\n",
    "            analyzer_template_path=template_path\n",
    "        )\n",
    "        result = content_understanding_client.poll_result(response)\n",
    "        print(f\"Successfully created analyzer: {analyzer_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create analyzer: {analyzer_id}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use created analyzers to extract multimodal content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use analyzer to extract document content with layout analysis\n",
    "#Iterate through each analyzer and analyze the content for each modality\n",
    "analyzer_results =[]\n",
    "extracted_markdown = []\n",
    "analyzer_content = []\n",
    "for analyzer in analyzer_configs:\n",
    "    analyzer_id = analyzer[\"id\"]\n",
    "    template_path = analyzer[\"template_path\"]\n",
    "    file_location = analyzer[\"location\"]\n",
    "    try:\n",
    "           # Analyze content\n",
    "            response = content_understanding_client.begin_analyze(analyzer_id, file_location)\n",
    "            result = content_understanding_client.poll_result(response)\n",
    "            analyzer_results.append({\"id\":analyzer_id, \"result\": result.get(\"result\", {})})\n",
    "            analyzer_content.append({\"id\": analyzer_id, \"content\": result.get(\"result\", {}).get(\"content\", [])})\n",
    "\n",
    "            # Extract markdown from the content list\n",
    "            extracted_markdown.append({\"id\": analyzer_id, \"markdown\": analyzer_content.get(\"content\", []).get(\"markdown\", \"\")})\n",
    "            print(f\"Markdown\", extracted_markdown.get(\"markdown\", \"\"))   \n",
    "             \n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error in creating analyzer. Please double-check your analysis settings.\\nIf there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\")\n",
    "\n",
    "            \n",
    "# Delete the analyzer if it is no longer needed\n",
    "#content_understanding_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize multimodal content extraction markdown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain separate indexes per modality, each optimized for its data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document content into semantic chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple starting point. Feel free to give your own chunking strategies a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.schema import Document\n",
    "# Configure langchain text splitting settings\n",
    "EMBEDDING_CHUNK_SIZE = 512\n",
    "EMBEDDING_CHUNK_OVERLAP = 20\n",
    "\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = extracted_markdown[0].get(\"markdown\", \"\") #first item is in extracted_markdown list is the document analyzer markdown output\n",
    "splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values_to_strings(json_obj):\n",
    "    return [str(value) for value in json_obj]\n",
    "\n",
    "\n",
    "#convert image content to JSON object\n",
    "def process_cu_image_output(contents):\n",
    "    image_splits = [\n",
    "        v for v in convert_values_to_strings(contents.get(\"content\", []))\n",
    "    ]\n",
    "    image_content = [Document(page_content=v) for v in image_splits]\n",
    "    return image_content\n",
    "\n",
    "#convert audio content to JSON object        \n",
    "def process_audio_output(contents):\n",
    "    audio_splits = [\n",
    "        v for v in convert_values_to_strings(contents.get(\"content\", []))\n",
    "    ]\n",
    "    audio_content = [Document(page_content=v) for v in audio_splits]\n",
    "    return audio_content\n",
    "\n",
    "#convert video content to JSON object\n",
    "def process_cu_video_scene_description(scene_description):\n",
    "    audio_visual_segments = scene_description.get(\"content\", [])\n",
    "    audio_visual_splits = [\n",
    "        \"The following is a json string representing a video segment with scene description and transcript ```\"\n",
    "        + v\n",
    "        + \"```\"\n",
    "        for v in convert_values_to_strings(audio_visual_segments)\n",
    "    ]\n",
    "    docs = [Document(page_content=v) for v in audio_visual_splits]\n",
    "    return docs\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result[\"result\"][\"contents\"])\n",
    "\n",
    "docs = process_cu_video_scene_description(video_cu_result)\n",
    "print(\"There are \" + str(len(docs)) + \" documents.\") \n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"doc content\", doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and index the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the splitted documents and insert into Azure Search vector store\n",
    "def embed_and_index_chunks(docs):\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "        openai_api_version=AZURE_OPENAI_EMBEDDING_API_VERSION,  # e.g., \"2023-12-01-preview\"\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "        azure_search_key=None,\n",
    "        index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "        embedding_function=aoai_embeddings.embed_query\n",
    "    )\n",
    "    vector_store.add_documents(documents=docs)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# embed and index the docs:\n",
    "vector_store = embed_and_index_chunks(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve relevant chunks based on a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant chunks based on the question\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(\n",
    "    \"<your question>\"\n",
    ")\n",
    "\n",
    "print(retrieved_docs[0].page_content)\n",
    "\n",
    "# Use a prompt for RAG that is checked into the LangChain prompt hub (https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=989ad331-949f-4bac-9694-660074a208a7)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=AZURE_OPENAI_CHAT_API_VERSION,  # e.g., \"2023-12-01-preview\"\n",
    "    azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the document\n",
    "\n",
    "rag_chain.invoke(\"<your question>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Q&A with references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the retrieved documents or certain source metadata from the documents\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"<your question>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
